{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Arithmetic Logit Units(NALU)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- 딥러닝 발달로 다양한 일을 할 수 있게됨\n",
    "- 그러나 systematic generalization은 글쎄...\n",
    "- systematic abstraction 보다는 memorization에 가깝다\n",
    "\n",
    "![Imgur](https://i.imgur.com/hXScZyC.png)\n",
    "\n",
    "- 논문에서는 일반적인 신경망에 적용할 수 있는 모듈을 만들었음\n",
    "- non-linearity 없이도 하나의 뉴런에 numerical quantitiy 표현\n",
    "- 미분도 가능해서 역전파 가능\n",
    "- Underlying numerical nature of data 파악\n",
    "- 과거 모델 마지막 단에만 적용해도 성능이 확 좋아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical extrapolation failures in NN\n",
    "\n",
    "![Imgur](https://i.imgur.com/rPg30Sf.png)\n",
    "\n",
    "$$ f(x)=x $$\n",
    "\n",
    "- 단순 identity function도 신경망은 제대로 학습하지 못함\n",
    "- 스칼라값을 input으로 가지는 오토인코더를 설계\n",
    "- 3 hidden layers of size 8, 10000 iterations, squared error loss\n",
    "- [-5,5]로 학습시키면 이 사이 값은 잘 맞추는데 [-20,20]은 못맞춤\n",
    "- PReLU같은 highly linear한 활성함수는 잘맞춤. 그러나 non-linear한 활성함수(tanh, sigmoid) 등은 형편없음)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAC, NALU\n",
    "\n",
    "- special case of a linear layer\n",
    "- NAC: 덧셈, 뺄셈\n",
    "- NALU : 곱셈, 나눗셈\n",
    "- NALU에는 gate-controlled sub opreaions가 있음\n",
    "\n",
    "![Imgur](https://i.imgur.com/ryzfWG7.png)\n",
    "\n",
    "![Imgur](https://i.imgur.com/fXQdJYw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def NALU(prev_layer, num_outputs):\n",
    "    eps=1e-7\n",
    "    shape = (int(prev_layer.shape[-1]),num_outputs)\n",
    "\n",
    "    # NAC cell\n",
    "    W_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    M_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    W = tf.tanh(W_hat) * tf.sigmoid(M_hat)\n",
    "    a = tf.matmul(prev_layer, W)\n",
    "    G = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    \n",
    "    # NALU\n",
    "    m = tf.exp(tf.matmul(tf.log(tf.abs(prev_layer) + eps), W))\n",
    "    g = tf.sigmoid(tf.matmul(prev_layer, G))\n",
    "    out = g * a + (1 - g) * m\n",
    "    return out\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Simple function learning tasks\n",
    "\n",
    "- 사칙연산, 제곱, 루트 등 학습\n",
    "- 입력값은 랜덤생성\n",
    "- interpolation, extrapolation 실험\n",
    "\n",
    "![Imgur](https://i.imgur.com/SfxbKcM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist counting and arithmetic tasks\n",
    "\n",
    "- 10개 MNIST digit 랜덤으로 뽑음\n",
    "- 어떤 digit이 관찰되었는지 마지막에 뽑아내야 함\n",
    "- MNIST Digit Counting task : 몇 개 봤는지 세야 함 (10-way regression)\n",
    "- MNIST Digit Addition task : compute the sum of the digits (a linear regression)\n",
    "- 곱셈 extrapolation이 잘 안되는건 아마 너무 작은 숫자가 등장해서 그런 듯(분모 들어가면 뻥튀기)\n",
    "\n",
    "![Imgur](https://i.imgur.com/taGu9I6.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to Number translation tasks\n",
    "\n",
    "- LSTM?\n",
    "- five hundred and fifteen -> 515\n",
    "- 0부터 1000까지 실험\n",
    "- Embedding layer -> LSTM -> NAC or NALU\n",
    "- LSTM은 과적합 됨\n",
    "- 모델이 'eighty one, eighty four, eighty seven'을 학습 중에 봤다면 'eighty'도 바로 맞춘다!\n",
    "\n",
    "\n",
    "\n",
    "![Imgur](https://i.imgur.com/0TLT4zQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program evaluation\n",
    "\n",
    "![Imgur](https://i.imgur.com/rnGmpJY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to track time in a Grid-World Environment\n",
    "\n",
    "- 정해진 시간 t에 정확히 빨간 네모에 도착하면 reward를 받음\n",
    "- 5 x 5 grid-world\n",
    "- 56 x 56 pixel input\n",
    "- action : up down left right pass\n",
    "- 정해진 시간 T 안에는 닿을 수 있게 실험 설계\n",
    "- T <= 19일 때 NAC합친 모델은 잘하지만 T > 13일 때 A3C 모델은 망가짐\n",
    "- 왜 agents가 결국 실패할까?\n",
    "  - stimuli greater than 12, the baseline agent behaves as if the stimulus were still 12\n",
    "  - 목적지에 t=12(너무 빠름)에 도착. larger stimuli보다 reward가 적음\n",
    "  - 반대로 stimuli가 20보다 크면 절대 도착 못함\n",
    "  - 다른 실험에 비해 extrapolation 결과가 좀 안좋은 것은 모델 자체가 여전히 LSTM을 사용하여 numeracy를 encode하기 때문\n",
    "  \n",
    "\n",
    "![Imgur](https://i.imgur.com/FIvZ6mX.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Parity Prediction Task & Ablation Study\n",
    "\n",
    "- 기존 네트워크에서 마지막 레이어만 바꿈\n",
    "- 바이어스 없애고 non linearity 추가하니까 더 잘됨\n",
    "\n",
    "![Imgur](https://i.imgur.com/Fv2uAID.png)\n",
    "\n",
    "![Imgur](https://i.imgur.com/icfjGpF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def NALU(prev_layer, num_outputs):\n",
    "    eps=1e-7\n",
    "    shape = (int(prev_layer.shape[-1]),num_outputs)\n",
    "\n",
    "    # NAC cell\n",
    "    W_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    M_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    W = tf.tanh(W_hat) * tf.sigmoid(M_hat)\n",
    "    a = tf.matmul(prev_layer, W)\n",
    "    G = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    \n",
    "    # NALU\n",
    "    m = tf.exp(tf.matmul(tf.log(tf.abs(prev_layer) + eps), W))\n",
    "    g = tf.sigmoid(tf.matmul(prev_layer, G))\n",
    "    out = g * a + (1 - g) * m\n",
    "    return out\n",
    "\n",
    "\n",
    "arithmetic_functions={\n",
    "'add': lambda x,y :x+y,\n",
    "}\n",
    "\n",
    "def get_data(N, op):\n",
    "    split = 4\n",
    "    X_train = np.random.rand(N, 10)*10\n",
    "    #to be mutually exclusive\n",
    "    a = X_train[:, :split].sum(1)\n",
    "    b = X_train[:, split:].sum(1)\n",
    "    Y_train = op(a, b)[:, None]\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    \n",
    "    X_test = np.random.rand(N, 10)*100\n",
    "    #to be mutually exclusive\n",
    "    a = X_test[:, :split].sum(1)\n",
    "    b = X_test[:, split:].sum(1)\n",
    "    Y_test = op(a, b)[:, None]\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return (X_train,Y_train),(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.01296727592485"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[0])\n",
    "#Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "(10000, 1)\n",
      "(10000, 10)\n",
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec424b1c871a406da1d1a2309543f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 12629128.0\n",
      "epoch 1000, loss: 0.0003893437678925693\n",
      "epoch 2000, loss: 0.00036618439480662346\n",
      "epoch 3000, loss: 0.000331127637764439\n",
      "epoch 4000, loss: 0.00028548159752972424\n",
      "epoch 5000, loss: 0.00023292415426112711\n",
      "epoch 6000, loss: 0.00017961060802917928\n",
      "epoch 7000, loss: 0.0002196947461925447\n",
      "epoch 8000, loss: 8.894406346371397e-05\n",
      "epoch 9000, loss: 11.192487716674805\n",
      "epoch 10000, loss: 3.6837584048043936e-05\n",
      "epoch 11000, loss: 2.1076919438201003e-05\n",
      "epoch 12000, loss: 1.602229349373374e-05\n",
      "epoch 13000, loss: 9.249086360796355e-06\n",
      "epoch 14000, loss: 4.6393128286581486e-05\n",
      "epoch 15000, loss: 0.0034365211613476276\n",
      "epoch 16000, loss: 3.094708517892286e-06\n",
      "epoch 17000, loss: 2.2120443645690102e-06\n",
      "epoch 18000, loss: 1.7752499843481928e-06\n",
      "epoch 19000, loss: 1.1439497029641643e-06\n",
      "epoch 20000, loss: 7.810526767570991e-07\n",
      "epoch 21000, loss: 5.993938430037815e-07\n",
      "epoch 22000, loss: 4.33683908340754e-07\n",
      "epoch 23000, loss: 4.783655640494544e-07\n",
      "epoch 24000, loss: 5.544134182855487e-07\n",
      "epoch 25000, loss: 7.16407157597132e-07\n",
      "epoch 26000, loss: 1.0835401553777047e-07\n",
      "epoch 27000, loss: 0.9918567538261414\n",
      "epoch 28000, loss: 6.82393874740228e-08\n",
      "epoch 29000, loss: 5.915217116125859e-08\n",
      "epoch 30000, loss: 9.73436726781074e-08\n",
      "epoch 31000, loss: 5.738911568187177e-08\n",
      "epoch 32000, loss: 1.1777774489019066e-07\n",
      "epoch 33000, loss: 6.213122105691582e-08\n",
      "epoch 34000, loss: 4.635694494936615e-08\n",
      "epoch 35000, loss: 4.73464751848951e-08\n",
      "epoch 36000, loss: 1.7882484826259315e-07\n",
      "epoch 37000, loss: 8.655524652567692e-08\n",
      "epoch 38000, loss: 1.2905729818157852e-07\n",
      "epoch 39000, loss: 4.518551577348262e-08\n",
      "epoch 40000, loss: 4.6073182602413e-08\n",
      "epoch 41000, loss: 5.286210580379702e-08\n",
      "epoch 42000, loss: 4.656430974137038e-08\n",
      "epoch 43000, loss: 4.393405106384307e-08\n",
      "epoch 44000, loss: 5.0118615035898983e-08\n",
      "epoch 45000, loss: 5.3158601076574996e-08\n",
      "epoch 46000, loss: 4.3612089939415455e-08\n",
      "epoch 47000, loss: 4.4708940549753606e-08\n",
      "epoch 48000, loss: 4.4086846173740923e-08\n",
      "epoch 49000, loss: 4.408502718433738e-08\n",
      "\n",
      "5.0640665e-06\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_examples=10000\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=get_data(train_examples,arithmetic_functions['add'])  \n",
    "X = tf.placeholder(tf.float32, shape=[train_examples, 10])\n",
    "Y = tf.placeholder(tf.float32, shape=[train_examples, 1])\n",
    "\n",
    "X_1=NALU(X,2)\n",
    "Y_pred=NALU(X_1,1)\n",
    "\n",
    "loss = tf.nn.l2_loss(Y_pred - Y) # NALU uses mse\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for ep in tqdm(range(50000)):\n",
    "        _,pred,l = session.run([train_op, Y_pred, loss], \n",
    "                feed_dict={X: X_train, Y: Y_train})\n",
    "        if ep % 1000 == 0:\n",
    "            print('epoch {0}, loss: {1}'.format(ep,l))\n",
    "\n",
    "    _,test_predictions,test_loss = session.run([train_op, Y_pred,loss],feed_dict={X:X_test,Y:Y_test})\n",
    "\n",
    "print(test_loss) #8.575397e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
