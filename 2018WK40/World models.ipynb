{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "![Imgur](https://i.imgur.com/Dc2Bx2t.png)\n",
    "\n",
    "“The image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system.”  \n",
    "  \n",
    "쉽게 말해, 사람들이 세상을 바라보고 이해할 때 세상 전체를 온전하게 받아들이는게 아니라 단순히 어떤 추상적인 컨셉, 관계 등을 이용한다는 것. 즉, 어떤 실제 사물을 바라보고 할 때 이를 'model'을 통해 이해합니다.\n",
    "  \n",
    "하루하루 어마어마한 양의 정보가 들어오면 뇌(model)은 이러한 정보들의 spatial & temporal한 추상적인 부분들을 가지고 학습을 진행합니다. 이렇게 들어온 정보들을 가지고 미래를 예측하는데 사용합니다.\n",
    "\n",
    "![What we see is based on our brain's prediction of the future](https://i.imgur.com/C6kuKhC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세상을 바라보는 시점 뿐만 아니라 어떤 'action'을 행할지 예측하는 모델 개념으로도 이해할 수 있습니다. 위험한 상황에 처했을 때 사람들은 의식적으로 다음 행동을 계획하지 않고 즉각적으로 반응합니다.  \n",
    "야구를 예로 들었을 때, 타자들은 야구 방망이를 어느 방향으로 휘두를지 밀리세컨드 안에 결정합니다. 이는 시각적인 정보가 뇌까지 들어오는 속도보다 짧습니다. 프로 야구선수들 같은 경우는 이런 식으로 예측하는 모델들과 근육들이 잘 훈련되었다고 할 수 있겠습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/u2O3pRV.png)\n",
    "\n",
    "\n",
    "강화학습 agent 관점에서 RNN은 과거, 현재를 잘 표현할 뿐만 아니라 미래에 대한 정보를 잘 예측할 수 있음이 알려져 있습니다. 이는 모델이 spatial & temporal representation을 잘 표현한다는 말과 같습니다. 그러나 이렇게 파라미터가 많은 모델을 학습할 경우 credit assignment problem이 발생할 수 있습니다. 따라서 보통 일반적인 강화학습들은 파라미터 수가 적은 model-free RL을 사용합니다. \n",
    "\n",
    "- credit assignment problem : 체스 경기 당 reward는 1개지만 이에 영향을 끼치는 수는 굉장히 많음. 그렇다면 어떤 수에 얼마만큼의 영향을 끼쳐야 할 것인가?\n",
    "\n",
    "논문에서는 RNN을 model로 활용하는 model-based RL 방법론을 제안합니다. 효과적인 학습을 위해 large model과 smaller controler model로 나누었습니다. Controller model의 경우에는 small search space에서 효율적으로 credit assignment problem을 잘 해결하는 것에 초점을 맞추었습니다.\n",
    "\n",
    "그리고 대부분의 model-based RL에서는 실제 environment를 가지고 학습을 진행합니다. 여기서는 서론에서 이야기 했듯이 실제 environment를 generate 되는 representation으로 대체하여 학습하는 실험도 진행하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Model\n",
    "\n",
    "![Imgur](https://i.imgur.com/3PWEREN.png)\n",
    "\n",
    "모델은 크게 다음과 같이 세 부분으로 구성되어 있습니다.\n",
    "\n",
    "- Vision(V) : VAE(Variational Auto Encoder)\n",
    "- Memory(M) : MDN-RNN(Mixture Density Networks - Recurrent Neural Network)\n",
    "- Controller(C) : simple single layer linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE (V) Model\n",
    "\n",
    "![Imgur](https://i.imgur.com/RVK37xN.png)\n",
    "\n",
    "먼저 environment의 고차원 데이터를 encode시키는 VAE입니다. 입력 데이터는 영상의 프레임이며 저차원으로 압축된 latent vector $z$를 사용합니다. VAE를 사용하는 이유?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDN-RNN (M) Model\n",
    "\n",
    "VAE가 입력 프레임을 압축시킨다면, MDN-RNN(이하 M)은 시간이 지남에 따라 어떤 상황이 발생하는지  예측합니다. 이렇게 sequential한 데이터를 사용하여 미래를 예측할 때는 RNN계열 네트워크들이 가장 효율적입니다. 논문에서는 RNN에 추가적으로 MDN(mixture density network)개념을 적용합니다. 이를 통해 RNN의 output으로 단순히 deterministic한 $z$값이 아닌 stochastic한 probability density function $p(z)$를 알 수 있습니다. 논문에서 이렇게 MDN을 사용하는 이유로 실제 복잡한 environment는 stochastic하기 때문이라고 설명하고 있습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/vQr8THN.png)\n",
    "\n",
    "MDN-RNN을 사용하면 V를 통해 encode된 $z$를 사용하여 다음 latent vector ${z}_{t+1}$의 probability distribution을 알아낼 수 있습니다. 수식으로 정리하면 다음과 같습니다.\n",
    "\n",
    "$$P({ z }_{ t+1 }|{ a }_{ t },{ z }_{ t },{ h }_{ t })$$\n",
    "\n",
    "- ${a}_{t}$ : action\n",
    "- ${z}_{t}$ : latent vector\n",
    "- ${h}_{t}$ : hidden state\n",
    "\n",
    "여기에 추가적으로 temperature parameter $\\tau $를 추가하여 모델의 uncertainty를 조절하였습니다. uncertainty가 높으면 높을수록 예측하는 프레임의 변동성이 높아지게 됩니다. (근데 어떤 방식으로 들어가는지 수식적으로 나온 부분이 없어서 적용 방식은 잘 모르겠습니다)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller (C) Model\n",
    "\n",
    "Controller는 environment가 지속적으로 변하는 상황에서 cumulative reward의 기대값을 최대화 하기 위한 action을 정합니다. 논문에서는 V와 M과 따로 학습을 시키기 위하여 일부러 최대한 파라미터 수가 적으면서도 간단한 모델을 만들었습니다. \n",
    "수식으로 나타내면 아래와 같습니다. $\\left[ { z }_{ t }{ h }_{ t } \\right]$는 concatenate vector이며 전체적으로 simple single layer linear model이라고 할 수 있습니다.\n",
    "\n",
    "$${ a }_{ t }={ W }_{ c }\\left[ { z }_{ t }{ h }_{ t } \\right] +{ b }_{ c }$$\n",
    "\n",
    "- ${a}_{t}$: action vector\n",
    "- ${W}_{c}$: weight matrix\n",
    "- ${b}_{c}$: bias vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리하면...\n",
    "\n",
    "종합적으로 정리하면 아래 다이어그램과 같이 나타낼 수 있습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/mz10c8H.png)\n",
    "\n",
    "Observation(프레임)이 들어오면 ${z}_{t}$를 생성합니다. ${z}_{t}$와 M의 hidden state ${h}_{t}$를 C에 집어넣고 action vector ${a}_{t}$를 계산합니다. 세 값 모두 합쳐 future latent vector ${z}_{t+1}$과 다음 프레임에 쓰일 hidden state ${h}+{t+1}$을 계산합니다. 슈도 코드로 나타내면 아래와 같습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def rollout(controller):\n",
    "  ''' env, rnn, vae are '''\n",
    "  ''' global variables  '''\n",
    "  obs = env.reset()\n",
    "  h = rnn.initial_state()\n",
    "  done = False\n",
    "  cumulative_reward = 0\n",
    "  while not done:\n",
    "    z = vae.encode(obs)\n",
    "    a = controller.action([z, h])\n",
    "    obs, reward, done = env.step(a)\n",
    "    cumulative_reward += reward\n",
    "    h = rnn.forward([a, z, h])\n",
    "  return cumulative_reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C가 매우 간단한 모델이기 때문에 practical한 장점이 있습니다. V와 M은 커다란 모델이므로 GPU를 사용하여 모델 학습 및 inference가 가능합니다. 따라서 많은 weight들이 필요한 역할은 V와 M에서 전부 끝내버릴 수 있습니다. 닭 잡는데 소 잡는 칼을 사용할 필요가 없는 것처럼, C를 학습하는 방법에도 굳이 GPU를 사용할 필요가 없습니다. \n",
    "\n",
    "논문에서는 유전 알고리즘 중 하나인 CMA-ES(Covatiance-Matrix Adaptation Evolution Strategy)를 사용하여 모델 회적화를 진행합니다. 이 알고리즘은 유동적으로 search space를 조절하는 장점을 가지고 있습니다. 파라미터 수가 수천개 정도로 적은 모델에서 많이 사용되는데, 논문에서는 CPU를 사용하여 파라미터를 학습시켰다고 합니다.\n",
    "    \n",
    "(제대로 된 controller 만들 때 어차피 파라미터 수가 많이 필요 없음 -> 그러면 굳이 GPU를 쓸필요 없음 -> CPU 사용하여 CMA-ES 방식 사용 -> 효율적인 분산 처리 효과)\n",
    "(근데 어차피 파라미터 수 적은데 메모리에 올려놓고 Gpu 쓰면 안되는건가?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Racing Experiment\n",
    "\n",
    "총 두가지 데이터를 가지고 실험을 진행하였습니다. 첫번째로는 CarRacing-v0데이터 입니다. 매 trial마다 랜덤하게 트랙이 생성되고, top-down 형식의 view입니다. 정해진 시간 안에 자동차는 최대한 회색의 트랙을 오랫동안 밟을수록 높은 reward를 받습니다. Agent(자동차)의 action벡터는 좌/우회전, 가속 그리고 브레이크로 이루어진 3차원의 벡터입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/O5yXUw5.png)\n",
    "\n",
    "먼저 V를 학습하기 위해 만 개의 environment를 수집하였습니다(만 개의 트랙이라 보는게 맞는듯?). 그리고 agent가 랜덤하게 탐색하며 여기서 관측되는 observation(프레임)을 사용하여 V를 학습시켰습니다(Reconstruction loss + KL loss를 사용한 것으로 보입니다). 아래 그림은 학습이 완료된 VAE로 다시 이미지를 재구성한 모습입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/3BUtJlr.png)\n",
    "\n",
    "이후에는 V를 통해 구한 ${z}_{t}$와 랜덤 액션 벡터 ${a}_{t}$를 사용하여 MDN-RNN을 학습하였습니다. $P({ z }_{ t+1 }|{ a }_{ t },{ z }_{ t },{ h }_{ t })$ 이 수식을 보면 MDN-RNN을 학습시키는데 필요한 요건이 갖추어진 것을 확인할 수 있습니다.\n",
    "\n",
    "V와 M만 가지고는 단순히 프레임을 압축하고 다음 프레임을 예측하는 것 밖에 못하기 때문에 실제 reward를 구할 수 없습니다. Reward에 직접적으로 영향을 끼치는 C는 파라미터 수가 867개로 매우 적기 때문에 CMA-ES를 사용하여 최적화 시킵니다. 전체적인 과정은 다음과 같습니다.\n",
    "\n",
    "\n",
    "1. Collect 10,000 rollouts from a random policy\n",
    "2. Train VAE(V) to encode each frame into a latent vector $z \\in \\mathcal{R}^{32}$\n",
    "3. Train MDN-RNN(M) to model $P({ z }_{ t+1 }|{ a }_{ t },{ z }_{ t },{ h }_{ t })$\n",
    "4. Evolve Controller(C) to maximize the expected cumulative reward of a rollout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Model Only\n",
    "\n",
    "사실 observation의 정보를 잘 담아낼 수 있는 representation만 가지고 있다면 agent를 학습시키는 것은 어려운 일이 아닙니다. 이전 강화학습 연구들도 이러한 방식으로 학습을 진행한 경우가 많았습니다. 추가적인 실험을 위해 논문에서도 M을 사용하지 않고 V만 사용하여 학습을 진행합니다. 수식으로 나타내면 아래와 같습니다.\n",
    "\n",
    "$${ a }_{ t }={ W }_{ c }{ z }_{ t } +{ b }_{ c }$$\n",
    "\n",
    "(홈피 참고)\n",
    "\n",
    "학습을 잘 하기는 하지만, 엄청 비틀거리면서 운행하며 급코너 구간에서는 트랙을 벗어나는 모습을 볼 수 있습니다. 100번의 실험을 진행하였을 때 $632\\pm 251$점을 얻었으며 이는 OpenAI Gym leaderboard의 다른 방법들과 비슷한 성능입니다. C에 추가적으로 hidden layer를 추가하면 $788\\pm 141$점으로 향상되기는 하였지만 여전히 아쉬운 면이 있습니다.\n",
    "\n",
    "\n",
    "### Full World Model (V and M)\n",
    "\n",
    "이번에는 V와 M모두 사용한 결과물입니다. V를 사용하면 현재 어떤 상황인지는 알 수 있지만 V와 M 모두 사용하면 현재 상황 뿐만 아니라 미래 상황에 대한 정보까지 C에 입력할 수 있습니다. \n",
    "\n",
    "(홈피 참고)\n",
    "\n",
    "확실히 비틀거리는 경우도 훨씬 덜하고 급커브도 부드럽게 돌면서 안정적인 운행을 하는 것을 확인할 수 있습니다. MDN-RNN의 장점, 즉 미래에 대한 확률 분포(${h}_{t}$) 덕분에 안정적인 action decision을 얻을 수 있습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/C4Ghok2.png)\n",
    "\n",
    "Full model을 사용하였더니 $906\\pm21$점이라는 매우 높은 SOTA급 득점 기록을 세울 수 있었다고 합니다. 기존 Deep RL 방법론들은 입력으로 들어오는 프레임들에 edge-detection등과 같은 전처리 기법과 여러 프레임을 하나로 쌓는 방식을 사용하였습니다. 그러나 논문에서 제안하는 방법론은 직접적으로 RGB 픽셀 값들을 집어넣어 spatial-temporal representation을 뽑아낼 수 있습니다. \n",
    "\n",
    "### Car Racing Dreams\n",
    "\n",
    "논문에서 제안하는 MDN-RNN은 매 프레임마다 다음 시점 프레임의 latent vector ${z}_{t+1}$를 예측합니다. 그렇다면 새로 관측되는 프레임을 encode하지 않고 예측한 다음 시점의 latent vector를 iterative하게 사용한다면 어떤 결과가 나올까요?\n",
    "\n",
    "(홈페이지)\n",
    "\n",
    "$\\tau$값을 변화시키는 것에 따라 생성되는 dream environment도 바뀜을 확인할 수 있습니다. 값을 높이면 uncertainty가 커져 더 추상적이고 뭉개지는 결과가 나옵니다. 그렇다면 이렇게 dream environment만 가지고 agent를 학습한 뒤에 실제 environment로 policy를 적용하면 잘 할수 있을까? 하는 궁금증이 생기게 됩니다.\n",
    "\n",
    "(이미지 트레이닝과 유사하다는 생각이 드네요)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VizDoom Experiment: Learning inside of a Dream\n",
    "\n",
    "서론에서 실제 사람들이 세상을 바라볼 때 전부를 똑같이 인식하는것이 아닌 'model'을 통해 추상화된 representation만 바라본다는 언급을 했습니다. 이 가정이 맞다면 방금 이야기 하였듯이 dream environment로 학습한 모델이 실제 세계에서도 잘 작동해야합니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/uKhmk5T.png)\n",
    "\n",
    "학습 데이터는 위와 같습니다. 하나의 rollout당 최대 60초(2100 time steps)까지 진행되는 환경이며 agent는 적이 발사하는 총알? fireball?을 좌 우로 움직이며 피해서 살아남아야 합니다. 20초(750 time steps)까지 살아남을 경우 생존하였다고 판단합니다.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "CarRacing 실험과 비슷하지만 몇 가지 다른 점이 있습니다. 이전 실험에서는 다음 latent vector ${Z}_{t}$만 예측해도 된다면, VizDoom에서는 다음 프레임에서 agent의 생존 여부 ${d}_{t}$도 예측하게 하였습니다. \n",
    "\n",
    "학습은 전부 latent space environment에서만 진행됩니다. 실제 environment와 동일한 구조를 가지고 있기 때문에 학습 완료 후에는 inference가 가능합니다. 전체적인 과정은 다음과 같습니다.\n",
    "\n",
    "1. Collect 10,000 rollouts from a random policy.\n",
    "2. Train VAE (V) to encode each frame into a latent vector $ z \\in \\mathcal{R}^{64}$, and use $V$ to convert the images collected from (1) into the latent space representation.\n",
    "3. Train MDN-RNN (M) to model $P({ z }_{ t+1 },{d}_{t+1}|{ a }_{ t },{ z }_{ t },{ h }_{ t })$\n",
    "4. Evolve Controller (C) to maximize the expected survival time inside the virtual environment.\n",
    "5. Use learned policy from (4) on actual Gym environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Inside of the Dream\n",
    "\n",
    "M에서 예측한 latent vector를 V를 사용하여 decode한 결과는 아래와 같습니다.\n",
    "\n",
    "(홈페이지 참고)\n",
    "\n",
    "RNN 모델은 게임 환경을 완전히 모사할 수 있게 학습이 됩니다. 무작위로 생성된 게임 환경의 이미지 데이터만 가지고 게임의 주요한 특징을 잘 잡아낼 수 있습니다. 예를 들어 agent가 왼쪽으로 움직이는 action을 선택한다면 M이 예측하는 dream environment 또한 agent를 왼쪽으로 움직이고 내부 환경을 그에 맞춰 변경시킵니다. 만약 fireball이 날아오는 상황이라면 해당 환경 또한 일관된 방향으로 날아오게끔 표현합니다. (미래를 일관성있게 잘 예측한다는 뜻) \n",
    "$\\tau$값을 바꾸면 불확실성을 많이 추가하여 실제 게임 환경보다 훨씬 어렵게 만들 수도 있습니다. 실험 결과 높은 uncertainty가 dream environment의 불완전한 부분에서 agent가 쉽게 학습하는것을 방지하여 더 좋은 결과가 나왔다고 합니다.\n",
    "\n",
    "\n",
    "### Cheating the World Model\n",
    "\n",
    "![Imgur](https://i.imgur.com/pQxYTfE.png)\n",
    "\n",
    "오락실 좀 가본 분들이라면 위 사진이 무엇인지 바로 아시겠지만, 게임 디자이너가 의도하지 않은 일명 얍삽이(?)입니다. 논문에서도 실험 초기에는 이러한 'generative policy' 현상이 발견되었다고 합니다. 특정 위치에서 약간씩 왔다갔다 하는 경우 fireball도 생성되지 않는 경우도 있고, agent가 특정 방향으로 이동할 경우 fireball이 사라져 버리는 경우도 있었습니다.\n",
    "\n",
    "(홈페이지 참고)\n",
    "\n",
    "이는 M이 사실은 environment의 probabilistic model을 단지 '근사'하기 때문이라고 합니다. 실제 게임 화면에서는 있어야 할 반대쪽 몬스터들이, dream에서는 없어져버리기도 합니다. 이는 모든 정보를 완벽히 가져갈 수 없는 한계라고도 할 수 있습니다.\n",
    "\n",
    "또한 C가 M의 hidden state 전부에 접근할 수 있으므로, 이는 agent가 게임 엔진의 모든 환경을 이해한다는 뜻입니다. 이러한 상황에서는 adversarial policy를 쉽게 찾아낼 수 있습니다. 더 쉽게 얘기하자면, 실제 학습 데이터의 분포(training distribution)에서 멀어져 버리게 됩니다. 이러한 이유 때문에 실제 관측 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
